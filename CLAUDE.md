# dynamodb-to-elasticsearch

## What This Is

An AWS Lambda function that syncs DynamoDB items to OpenSearch (formerly Elasticsearch) in near-realtime. Processes DynamoDB Streams events (INSERT, MODIFY, REMOVE) and indexes/updates/deletes corresponding documents in OpenSearch. Ensures search indexes stay in sync with DynamoDB tables.

**Use case**: Enable full-text search on DynamoDB data without dual-write logic in application code.

## Tech Stack

- **Language**: Python 3.x (originally 2.7, should be upgraded)
- **AWS Services**: Lambda, DynamoDB Streams, OpenSearch/Elasticsearch
- **Dependencies**:
  - `boto3` — AWS SDK for Python
  - `elasticsearch` or `opensearch-py` — OpenSearch/Elasticsearch client
  - `requests-aws4auth` — AWS Signature V4 signing for OpenSearch requests
- **Build tool**: `Makefile` — packaging, deployment, mapping generation

## Quick Start

```bash
# Setup
# 1. Create OpenSearch cluster in AWS
# 2. Enable DynamoDB Streams on tables you want to index
# 3. Create IAM role for Lambda with DynamoDB Streams and OpenSearch permissions

# Configure environment
export AWS_BUCKET_CODE=my-lambda-bucket
export IAM_ROLE=arn:aws:iam::123456789012:role/DynamoToESRole
export ENV=DEV
export PROFILE=default  # Optional

# Create config file and upload to S3
echo "ES_ENDPOINT='https://search-cluster.region.es.amazonaws.com'" > DEV_es_creds
aws s3 cp DEV_es_creds s3://my-lambda-bucket/DEV_es_creds

# Generate table mapping (ensures correct key order)
./update_mapping.py

# Create Lambda function (first time)
make create/DynamoToES DESC="Process DynamoDB stream to ES"

# Update Lambda function (after code changes)
make deploy/DynamoToES DESC="Process DynamoDB stream to ES"

# Configure DynamoDB Stream triggers
# Go to AWS Lambda console → DynamoToES function → Add trigger → DynamoDB Stream
```

## Project Structure

- `src/` — Lambda function source code
  - `src/DynamoToES/index.py` — Main handler function
- `lib/` — Shared libraries
  - `lib/env.py` — Environment config (generated from S3 at build time)
  - `lib/table_mapping.json` — DynamoDB table key mappings (generated by `update_mapping.py`)
- `dist/` — Build output (ZIP files)
- `Makefile` — Build, package, deploy automation
- `update_mapping.py` — Script to generate table mappings
- `requirements.txt` — Python dependencies

## Dependencies

**External:**
- AWS DynamoDB Streams — source of events
- AWS OpenSearch Service — destination for indexed documents
- AWS Lambda — execution environment
- AWS S3 — storage for Lambda ZIP and config files

**Internal:**
- `lib/env.py` — Config file generated from S3-stored `${ENV}_es_creds`
- `lib/table_mapping.json` — Table key mapping generated by `update_mapping.py`

## API / Interface

**Input**: DynamoDB Stream event (JSON). Lambda is triggered automatically when DynamoDB items are inserted, modified, or deleted.

**Output**: Documents indexed in OpenSearch. One ES operation per DynamoDB Stream record.

**Event types processed:**
- `INSERT` — Index new document in OpenSearch
- `MODIFY` — Update existing document in OpenSearch
- `REMOVE` — Delete document from OpenSearch

**Index naming**: OpenSearch index name matches DynamoDB table name (lowercase).

**Document ID**: DynamoDB primary key (hash + range if composite) is used as OpenSearch document `_id`.

## Key Patterns

- **Event-driven**: Triggered by DynamoDB Streams, not polling
- **Stateless**: Each Lambda invocation processes one batch of Stream records independently
- **Force index refresh**: Uses `refresh=true` for near-realtime indexing (trade-off: slightly higher latency)
- **JSON unmarshaling**: DynamoDB JSON format (with types like `S`, `N`, `L`) is unmarshaled to standard JSON
- **List type normalization**: Lists of mixed types (numbers, strings) are converted to lists of strings (ES doesn't support mixed-type arrays)
- **Table mapping**: Uses `lib/table_mapping.json` to ensure correct key order (DynamoDB Streams doesn't guarantee key order)

## Environment

**Required environment variables** (from `${ENV}_es_creds` file in S3):
- `ES_ENDPOINT` — OpenSearch cluster endpoint URL (e.g., `https://search-cluster.region.es.amazonaws.com`)

**Required IAM permissions** (Lambda execution role):
- DynamoDB Streams: `dynamodb:DescribeStream`, `dynamodb:GetRecords`, `dynamodb:GetShardIterator`, `dynamodb:ListStreams`
- OpenSearch: `es:ESHttpGet`, `es:ESHttpPut`, `es:ESHttpPost`, `es:ESHttpDelete`
- CloudWatch Logs: `logs:CreateLogGroup`, `logs:CreateLogStream`, `logs:PutLogEvents`

**Makefile environment variables:**
- `AWS_BUCKET_CODE` — S3 bucket for Lambda ZIP storage
- `IAM_ROLE` — Lambda execution role ARN
- `ENV` — Environment name (DEV, QA, PROD) — used to load correct config file
- `PROFILE` — AWS CLI profile (optional)

## Deployment

**First-time setup:**
1. Create OpenSearch cluster
2. Create S3 bucket for Lambda code
3. Create IAM role with DynamoDB Streams and OpenSearch permissions
4. Create config file `${ENV}_es_creds` with `ES_ENDPOINT` variable
5. Upload config file to S3: `aws s3 cp ${ENV}_es_creds s3://${AWS_BUCKET_CODE}/`
6. Generate table mapping: `./update_mapping.py`
7. Create Lambda function: `make create/DynamoToES DESC="Sync DynamoDB to OpenSearch"`
8. Configure DynamoDB Stream triggers in Lambda console

**Updates:**
1. Make code changes
2. Regenerate table mapping if table schemas changed: `./update_mapping.py`
3. Deploy: `make deploy/DynamoToES DESC="Update sync logic"`

**Table mapping updates:**
- Run `./update_mapping.py` whenever DynamoDB table keys change
- Script queries all DynamoDB tables with Streams enabled
- Generates `lib/table_mapping.json` with primary key metadata
- Mapping is bundled into Lambda ZIP

## Testing

<!-- Ask: Does this repo have unit tests? -->

**Manual testing:**
1. Insert/update/delete items in a DynamoDB table with Streams enabled
2. Check CloudWatch Logs for Lambda execution logs
3. Query OpenSearch to verify documents are indexed correctly
4. Verify `INSERT`, `MODIFY`, and `REMOVE` events are handled

**CloudWatch Logs**: Monitor for errors, retries, and processing times.

## Gotchas

- **Key order dependency**: DynamoDB Streams doesn't guarantee key order in composite primary keys. The table mapping fixes this, but it MUST be regenerated when table schemas change.
- **No batching**: Lambda processes each Stream record individually. For high-throughput tables, this can be slow. Consider batching ES requests.
- **Force refresh overhead**: `refresh=true` on every ES operation adds latency. For lower latency, remove `refresh=true` and accept eventual consistency (~1 second delay).
- **Binary types untested**: DynamoDB binary (`B`) types have not been tested. May fail or index as base64.
- **Mixed-type arrays**: Lists with mixed types (e.g., `[1, "foo", true]`) are converted to strings `["1", "foo", "true"]`. This may break queries expecting numeric arrays.
- **ES mapping conflicts**: If DynamoDB field types change (e.g., number → string), ES may reject the update due to mapping conflicts. You'd need to reindex or use a new index.
- **No retry logic**: If ES indexing fails, Lambda retries automatically (DynamoDB Streams guarantees at-least-once delivery), but there's no custom retry logic.
- **Stream retention**: DynamoDB Streams retain records for 24 hours. If Lambda is down for >24 hours, you'll miss updates. Implement a backfill mechanism if needed.
- **Lambda concurrency**: One Lambda invocation per shard. For high-throughput tables with many shards, ensure Lambda concurrency limits are set appropriately.
- **Table name as index**: OpenSearch index name is the DynamoDB table name (lowercase). If you rename a table, you'll need to reindex manually.

<!-- Ask: Is there a backfill mechanism if Lambda misses events due to downtime? -->
<!-- Ask: Has this been migrated to OpenSearch, or does it still use Elasticsearch client? -->
<!-- Ask: Are there any performance benchmarks for high-throughput tables? -->
<!-- Ask: Should we batch ES operations for better performance? -->