# dynamodb-to-elasticsearch

## What This Is

An AWS Lambda function that syncs DynamoDB items to OpenSearch (formerly Elasticsearch) in near-realtime. Processes DynamoDB Streams events (INSERT, MODIFY, REMOVE) and indexes/updates/deletes corresponding documents in OpenSearch. Ensures search indexes stay in sync with DynamoDB tables.

**Use case**: Enable full-text search on DynamoDB data without dual-write logic in application code.

**Companion service**: `ImportToOpensearch` Lambda in `docker-environments/cloudformation/Lambdas/` handles bulk backfill (full table re-index). This repo handles the continuous stream sync.

## Tech Stack

- **Language**: Python 3.12 (runtime set in Makefile)
- **AWS Services**: Lambda, DynamoDB Streams, OpenSearch Service (formerly Elasticsearch)
- **Dependencies**:
  - `boto3` -- AWS SDK for Python
  - `elasticsearch==7.17.9` -- **WARNING: Not yet migrated to `opensearch-py`**. See Migration Status below.
  - `requests-aws4auth==1.2.3` -- AWS Signature V4 signing for OpenSearch requests
- **Build tool**: `Makefile` -- packaging, deployment, mapping generation
- **Architecture**: arm64 (set in Makefile deploy target)

## Stream Architecture

```
DynamoDB Table
    |
    | (DynamoDB Streams - NEW_AND_OLD_IMAGES or NEW_IMAGE)
    v
Lambda: DynamoToES
    |
    | (one HTTP call per record - no bulk API)
    v
OpenSearch Service
    |
    | Index = table name (lowercase)
    | Doc ID = PrimaryKey[|SortKey]
    v
Search queries from sa_site_v2 / APIs
```

**Data flow per event:**
1. DynamoDB Stream delivers batch of records to Lambda (batch size configured in trigger)
2. Lambda iterates records sequentially
3. For each record: unmarshal DynamoDB JSON -> standard JSON -> index/update/delete in OpenSearch
4. OpenSearch reserved fields (`_id`, `_type`, etc.) are escaped: `_` -> `__` prefix
5. Index is force-refreshed after each operation (`refresh=True`)

**Companion services:**
- `ImportToOpensearch` (docker-environments) -- Bulk re-index entire tables. Disables stream during import, creates versioned indices with alias swapping.
- `ListTableWithDynamoStream` (docker-environments) -- Lists all tables connected to DynamoToES.
- `update_mapping.py` (this repo) -- Generates key schema mapping to fix DynamoDB Streams key ordering issue.

## Quick Start

```bash
# Setup
# 1. Create OpenSearch cluster in AWS
# 2. Enable DynamoDB Streams on tables you want to index
# 3. Create IAM role for Lambda with DynamoDB Streams and OpenSearch permissions

# Configure environment
export AWS_BUCKET_CODE=my-lambda-bucket
export IAM_ROLE=arn:aws:iam::123456789012:role/DynamoToESRole
export AWSENV_NAME=dev  # NOTE: Makefile uses AWSENV_NAME, not ENV
export PROFILE=default  # Optional

# Create config file and upload to S3
echo "ES_ENDPOINT='https://search-cluster.region.es.amazonaws.com'" > dev_es_creds
aws s3 cp dev_es_creds s3://my-lambda-bucket/dev_es_creds

# Generate table mapping (ensures correct key order)
./update_mapping.py

# Create Lambda function (first time)
make create/DynamoToES DESC="Process DynamoDB stream to ES"

# Update Lambda function (after code changes)
make deploy/DynamoToES DESC="Process DynamoDB stream to ES"

# Configure DynamoDB Stream triggers
# Go to AWS Lambda console -> DynamoToES function -> Add trigger -> DynamoDB Stream
```

## Project Structure

- `src/` -- Lambda function source code
  - `src/DynamoToES/index.py` -- Main handler function (single file, ~218 lines)
- `lib/` -- Shared libraries
  - `lib/env.py` -- Environment config (generated from S3 at build time, gitignored)
  - `lib/table_mapping.json` -- DynamoDB table key mappings (generated by `update_mapping.py`, gitignored)
  - `lib/__init__.py` -- Package init
- `dist/` -- Build output (ZIP files)
- `Makefile` -- Build, package, deploy automation
- `update_mapping.py` -- Script to generate table mappings from live DynamoDB
- `requirements.txt` -- Python dependencies

## Dependencies

**External:**
- AWS DynamoDB Streams -- source of events
- AWS OpenSearch Service -- destination for indexed documents
- AWS Lambda -- execution environment
- AWS S3 -- storage for Lambda ZIP and config files (`${AWSENV_NAME}_es_creds`)

**Internal:**
- `lib/env.py` -- Config file generated from S3-stored `${AWSENV_NAME}_es_creds`
- `lib/table_mapping.json` -- Table key mapping generated by `update_mapping.py`

## API / Interface

**Input**: DynamoDB Stream event (JSON). Lambda is triggered automatically when DynamoDB items are inserted, modified, or deleted.

**Output**: Documents indexed in OpenSearch. One ES operation per DynamoDB Stream record.

**Event types processed:**
- `INSERT` -- Index new document in OpenSearch. Auto-creates index if missing.
- `MODIFY` -- Full reindex of document (not partial update).
- `REMOVE` -- Delete document from OpenSearch.

**Index naming**: OpenSearch index name = DynamoDB table name (lowercase).

**Document ID format**: `{PrimaryKeyValue}` or `{PrimaryKeyValue}|{SortKeyValue}` for composite keys.

**Reserved field escaping**: ES reserved fields (`_id`, `_type`, `_source`, `_all`, `_parent`, `_fieldnames`, `_routing`, `_index`, `_size`, `_timestamp`, `_ttl`, `uid`) have first `_` replaced with `__`.

## Key Patterns

- **Event-driven**: Triggered by DynamoDB Streams, not polling
- **Stateless**: Each Lambda invocation processes one batch of Stream records independently
- **Force index refresh**: Uses `refresh=true` for near-realtime indexing (trade-off: significant performance overhead under load)
- **JSON unmarshaling**: DynamoDB JSON format (with types like `S`, `N`, `L`) is unmarshaled to standard JSON
- **Number coercion**: Numbers (`N` type) are converted to int or float depending on value
- **Table mapping**: Uses `lib/table_mapping.json` to ensure correct key order (DynamoDB Streams does not guarantee key order in composite keys)
- **Error handling**: Per-record try/catch with `continue` -- **WARNING: failed records are silently dropped, not retried**

## Environment

**Required environment variables** (from `${AWSENV_NAME}_es_creds` file in S3):
- `ES_ENDPOINT` -- OpenSearch cluster endpoint URL (e.g., `https://search-cluster.region.es.amazonaws.com`)

**Required IAM permissions** (Lambda execution role):
- DynamoDB Streams: `dynamodb:DescribeStream`, `dynamodb:GetRecords`, `dynamodb:GetShardIterator`, `dynamodb:ListStreams`
- OpenSearch: `es:ESHttpGet`, `es:ESHttpPut`, `es:ESHttpPost`, `es:ESHttpDelete`
- CloudWatch Logs: `logs:CreateLogGroup`, `logs:CreateLogStream`, `logs:PutLogEvents`

**Makefile environment variables:**
- `AWS_BUCKET_CODE` -- S3 bucket for Lambda ZIP storage
- `IAM_ROLE` -- Lambda execution role ARN
- `AWSENV_NAME` -- Environment name (dev, dev-eu, qa, prod) -- used to load correct config file from S3
- `PROFILE` -- AWS CLI profile (optional)
- `SUBNET_IDS` -- VPC subnet IDs for Lambda (required for `create` target)
- `SG_IDS` -- Security group IDs for Lambda (required for `create` target)

## Deployment

**First-time setup:**
1. Create OpenSearch cluster
2. Create S3 bucket for Lambda code
3. Create IAM role with DynamoDB Streams and OpenSearch permissions
4. Create config file `${AWSENV_NAME}_es_creds` with `ES_ENDPOINT` variable
5. Upload config file to S3: `aws s3 cp ${AWSENV_NAME}_es_creds s3://${AWS_BUCKET_CODE}/`
6. Generate table mapping: `./update_mapping.py`
7. Create Lambda function: `make create/DynamoToES DESC="Sync DynamoDB to OpenSearch"`
8. Configure DynamoDB Stream triggers in Lambda console

**Updates:**
1. Make code changes
2. Regenerate table mapping if table schemas changed: `./update_mapping.py`
3. Deploy: `make deploy/DynamoToES`

**Table mapping updates:**
- Run `./update_mapping.py` whenever DynamoDB table keys change
- Script queries all DynamoDB tables with Streams enabled via `DynamoToES` event source mappings
- Generates `lib/table_mapping.json` with primary key and sort key metadata
- Mapping is bundled into Lambda ZIP
- **Known issue**: Script regex `[a-zA-Z]+` misses tables with numbers/underscores/hyphens

## Migration Status: Elasticsearch to OpenSearch

**Current state: NOT MIGRATED**

| Component | Status | Notes |
|-----------|--------|-------|
| Python client | `elasticsearch==7.17.9` | Should be `opensearch-py>=2.4.0` |
| Import statements | `from elasticsearch import ...` | Should be `from opensearchpy import ...` |
| Client class | `Elasticsearch()` | Should be `OpenSearch()` |
| `doc_type` parameter | Used on all index/delete calls | Removed in OpenSearch 2.x |
| Service signing | `'es'` | Correct for AWS OpenSearch Service |
| Config variable | `ES_ENDPOINT` | Works but misleading name |

**The companion `ImportToOpensearch` Lambda has already been migrated.** This repo is the last holdout.

## Testing

No automated tests exist in this repo.

**Manual testing:**
1. Insert/update/delete items in a DynamoDB table with Streams enabled
2. Check CloudWatch Logs for Lambda execution logs
3. Query OpenSearch to verify documents are indexed correctly
4. Verify `INSERT`, `MODIFY`, and `REMOVE` events are handled

**CloudWatch Logs**: Monitor for errors, retries, and processing times.

## Gotchas

- **Silent data loss**: Failed records are caught, logged, and skipped (`continue`). The Lambda reports success, so DynamoDB Streams advances the iterator. Failed records are never retried. This is the most dangerous behavior in the codebase.
- **Not migrated to OpenSearch client**: Still uses `elasticsearch` library. Will break if ES client enforces license checks or if cluster upgrades to OpenSearch 2.x remove `doc_type` support.
- **`doc_type` deprecated**: All `es.index()` and `es.delete()` calls pass `doc_type=table`, which is removed in OpenSearch 2.x.
- **`refresh=True` on every operation**: Forces index refresh after each record. Kills performance under load. The 1-second default refresh interval is sufficient for near-realtime.
- **No bulk API**: Each record is a separate HTTP call. DynamoDB Streams batches records, but this Lambda processes them one at a time.
- **Key order dependency**: DynamoDB Streams doesn't guarantee key order in composite primary keys. The table mapping fixes this, but it MUST be regenerated when table schemas change.
- **Race condition on index creation**: Concurrent Lambda invocations can both try to create the same index. Second call will fail.
- **Binary types untested**: DynamoDB binary (`B`) type has no handler -- values silently become `None`. Binary sets (`BS`) are handled alongside lists.
- **Unknown types become None**: `unmarshalValue` has no catch-all -- new DynamoDB types will silently produce `null` values.
- **PII in logs**: Full document bodies are printed to CloudWatch. Fan data likely contains PII.
- **Mapping regex mismatch**: `update_mapping.py` uses `[a-zA-Z]+` but `index.py` uses `[0-9a-zA-Z_-]+`. Tables with numbers in their name are silently excluded from mappings.
- **AWSENV_NAME vs ENV**: Makefile checks `AWSENV_NAME` but README says `ENV`. Use `AWSENV_NAME`.
- **Hardcoded function name**: `update_mapping.py` hardcodes `FunctionName='DynamoToES'`.
- **Reserved field escaping is one-way**: `_id` -> `__id` in OpenSearch, but nothing unescapes on read. Downstream consumers must know about this.
- **ES client init per invocation**: Session, auth, and client are recreated every invocation instead of being reused across warm starts.
- **Lambda timeout**: 10 seconds (Makefile). Large batches with many records may timeout, especially with per-record HTTP calls and `refresh=True`.
- **GitHub backup workflow**: Triggers on `develop` branch but default branch is `master`. Backup never runs.
